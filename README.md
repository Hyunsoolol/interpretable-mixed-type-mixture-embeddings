# Interpretable Mixed-type Mixture Modeling (IM3)

![Repo Name](https://img.shields.io/badge/Repo-interpretable--mixed--mixture-blueviolet)
![Status](https://img.shields.io/badge/Status-Research%20Proposal-blue)
![Python](https://img.shields.io/badge/Python-3.9%2B-green)
![Topic](https://img.shields.io/badge/Topic-Mixture%20Models%20%7C%20NLP%20%7C%20XAI-orange)

## ðŸ“Œ Overview

This project proposes a novel statistical framework for clustering **mixed-type data** composed of categorical variables (e.g., demographics) and high-dimensional text data. 

Unlike traditional approaches that rely on Bag-of-Words (BoW) and strong independence assumptions (Naive Bayes), this study leverages **Large Language Model (LLM) embeddings** to capture semantic context. To address the "Black-box" nature of embeddings and the "Curse of dimensionality," we introduce a **De-embedding** strategy for model interpretation and a dimensionality reduction step for stable Gaussian Mixture modeling.

## ðŸ“– Motivation

### The Limitation of Existing Methods
Recent studies, such as [Shi et al. (2024)](https://doi.org/10.1214/24-AOAS1893), utilize **Mixture Conditional Regression (MCR)** to estimate extralegal factors in judicial decisions. However, these methods typically model text data $Z$ as a binary vector under a **Naive Bayes assumption**:

$$
P(Z_i | K_i = k) = \prod_{j=1}^{p} P(Z_{ij} | K_i = k)
$$

**Critique:**
1.  **Loss of Semantics:** Binary indicators ignore word order, context, and semantic nuance.
2.  **Unrealistic Assumption:** The independence assumption between words is often violated in natural language.

### Our Approach
We propose replacing the binary feature vector with **dense embeddings** generated by LLMs (e.g., SBERT, OpenAI). This transforms the problem into a **Mixed-type Mixture Model** (Categorical + Continuous). To ensure interpretability and computational feasibility, we integrate dimensionality reduction and a post-hoc "De-embedding" analysis.

## ðŸ› ï¸ Methodology

The proposed framework consists of three main stages:

```mermaid
graph LR
    A[Raw Data] --> B{Preprocessing};
    B -->|Categorical X_cat| C[One-Hot Encoding];
    B -->|Text Z| D[LLM Embedding];
    D -->|High-dim Vector| E["Dim Reduction (PCA/UMAP)"];
    E -->|"Reduced Vector X_cont"| F[Joint Mixture Modeling];
    C --> F;
    F --> G[Latent Class Identification];
    G --> H[Interpretation via De-embedding];
